{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pkg_resources\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ActflowToolbox as actflow\n",
    "from nltools.utils import get_resource_path\n",
    "from nltools.file_reader import onsets_to_dm\n",
    "from nltools.data import Design_Matrix\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thetarfile = \"https://osf.io/bqp7m/download/\"\n",
    "# ftpstream = urllib.request.urlopen(thetarfile)\n",
    "# thetarfile = tarfile.open(fileobj=ftpstream, mode=\"r|gz\")\n",
    "# thetarfile.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thetarfile = \"https://osf.io/bqp7m/download/\"\n",
    "# ftpstream = urllib.request.urlopen(thetarfile)\n",
    "# thetarfile = tarfile.open(fileobj=ftpstream, mode=\"r|gz\")\n",
    "# thetarfile.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data shared for NMA projects is a subset of the full HCP dataset\n",
    "N_SUBJECTS = 339\n",
    "\n",
    "# The data have already been aggregated into ROIs from the Glasesr parcellation\n",
    "N_PARCELS = 360\n",
    "\n",
    "# The acquisition parameters for all tasks were identical\n",
    "TR = 0.72  # Time resolution, in sec\n",
    "\n",
    "# The parcels are matched across hemispheres with the same order\n",
    "HEMIS = [\"Right\", \"Left\"]\n",
    "\n",
    "# Each experiment was repeated multiple times in each subject\n",
    "N_RUNS_REST = 4\n",
    "N_RUNS_TASK = 2\n",
    "\n",
    "# Time series data are organized by experiment, with each experiment\n",
    "# having an LR and RL (phase-encode direction) acquistion\n",
    "BOLD_NAMES = [\n",
    "  \"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\",\n",
    "  \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\",\n",
    "  \"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_LR\",\n",
    "  \"tfMRI_WM_RL\", \"tfMRI_WM_LR\",\n",
    "  \"tfMRI_EMOTION_RL\", \"tfMRI_EMOTION_LR\",\n",
    "  \"tfMRI_GAMBLING_RL\", \"tfMRI_GAMBLING_LR\",\n",
    "  \"tfMRI_LANGUAGE_RL\", \"tfMRI_LANGUAGE_LR\",\n",
    "  \"tfMRI_RELATIONAL_RL\", \"tfMRI_RELATIONAL_LR\",\n",
    "  \"tfMRI_SOCIAL_RL\", \"tfMRI_SOCIAL_LR\"\n",
    "]\n",
    "\n",
    "\n",
    "#Dictionaries \n",
    "conditions_dict={\n",
    "    \"motor\": [\"cue\", \"rf\", \"lf\", \"rh\", \"lh\"],\n",
    "    \"wm\": [\"0bk_body\", \"0bk_faces\", \"0bk_places\", \"0bk_tools\", \"2bk_body\", \n",
    "           \"2bk_faces\", \"2bk_places\", \"2bk_tools\"],\n",
    "    \"emotion\": [\"fear\", \"neut\"],\n",
    "    \"gambling\": [\"win\", \"loss\"],\n",
    "    \"language\": [\"story\", \"math\"],\n",
    "    \"relational\": [\"match\", \"relation\"],\n",
    "    \"social\": [\"mental\", \"rnd\"]}\n",
    "\n",
    "run_length_dict = {\n",
    "    \"motor\": 284,\n",
    "    \"wm\": 405,\n",
    "    \"emotion\": 176,\n",
    "    \"gambling\": 253,\n",
    "    \"language\": 316,\n",
    "    \"relational\": 232,\n",
    "    \"social\": 274}\n",
    "\n",
    "bold_name_dict = {\n",
    "    \"rest\": [\"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\", \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\"],\n",
    "    \"motor\": [\"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_LR\"],\n",
    "    \"wm\": [\"tfMRI_WM_RL\", \"tfMRI_WM_LR\"],\n",
    "    \"emotion\": [\"tfMRI_EMOTION_RL\", \"tfMRI_EMOTION_LR\"],\n",
    "    \"gambling\": [\"tfMRI_GAMBLING_RL\", \"tfMRI_GAMBLING_LR\"],\n",
    "    \"language\": [\"tfMRI_LANGUAGE_RL\", \"tfMRI_LANGUAGE_LR\"],\n",
    "    \"relational\": [\"tfMRI_RELATIONAL_RL\", \"tfMRI_RELATIONAL_LR\"],\n",
    "    \"social\": [\"tfMRI_SOCIAL_RL\", \"tfMRI_SOCIAL_LR\"]}\n",
    "\n",
    "task_run_dict = {\n",
    "    \"rest\": [1,2,3,4],\n",
    "    \"motor\": [5,6],\n",
    "    \"wm\": [7,8],\n",
    "    \"emotion\": [9,10],\n",
    "    \"gambling\": [11,12],\n",
    "    \"language\": [13, 14],\n",
    "    \"relational\": [15, 16],\n",
    "    \"social\": [17, 18]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cond_evs(cond_name, task_name, subject, run = 1):\n",
    "  \"\"\"Load onset files for a single condition from a task.\n",
    "  \n",
    "  Args:\n",
    "    cond_name (str): condition name pulled from the conditions_dict for task_name\n",
    "    task_name (str): task name instead of the bold run\n",
    "    subject (int): 0-based subject ID to load\n",
    "    run (int): 1 or 2 for task runs (use run-1 for indexing)\n",
    "\n",
    "  Returns\n",
    "    cond_evs (n_blocks x 3): Events file for single condition to be formatted for design matrix\n",
    "\n",
    "  \"\"\"\n",
    "  bold_name = bold_name_dict[task_name][run-1]\n",
    "  cond_evs = pd.read_csv('%s/subjects/%s/EVs/%s/%s.txt'%(HCP_DIR, subject, bold_name, cond_name), sep=\"\\t\", header=None)\n",
    "  cond_evs = cond_evs.rename(columns={0: \"Onset\", 1: \"Duration\", 2: \"amplitude\"})\n",
    "  cond_evs = cond_evs.drop(columns=['amplitude'])\n",
    "  cond_evs['Stim'] = cond_name\n",
    "\n",
    "  return cond_evs\n",
    "\n",
    "def get_run_evs(subject, task_name, run = 1):\n",
    "  \"\"\"Load onset files for a full file.\n",
    "  \n",
    "  Args:\n",
    "    task_name (str): task name instead of the bold run\n",
    "    subject (int): 0-based subject ID to load\n",
    "    run (int): 1 or 2 for task runs (use run-1 for indexing)\n",
    "\n",
    "  Returns\n",
    "    evs (n_blocks for run x 3 array): Events file for single condition to be formatted for design matrix\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  conditions = conditions_dict[task_name]\n",
    "\n",
    "  evs = pd.DataFrame()\n",
    "\n",
    "  for cond in conditions:\n",
    "    cond_evs = get_cond_evs(cond, task_name, subject, run)\n",
    "    evs = evs.append(cond_evs)\n",
    "  \n",
    "  evs = evs.sort_values(by=\"Onset\") \n",
    "\n",
    "  return evs\n",
    "\n",
    "def run_evs_to_dm(run_evs, task_name, TR=.72, convolve = True, add_poly = 2, dct_basis=False):\n",
    "\n",
    "  sampling_freq = 1./TR\n",
    "  run_length = run_length_dict[task_name]\n",
    "  dm = onsets_to_dm(run_evs, sampling_freq=sampling_freq, run_length=run_length, sort=True, add_poly=add_poly)\n",
    "\n",
    "  if convolve: \n",
    "    dm = dm.convolve()\n",
    "\n",
    "  if dct_basis:\n",
    "    dm = dm.add_dct_basis()\n",
    "\n",
    "  return dm\n",
    "\n",
    "def get_task_dms(subject, task_name, TR = .72, convolve = True, add_poly = 2, dct_basis=False):\n",
    "\n",
    "  runs = list(range(1,len(task_run_dict[task_name])+1))\n",
    "  task_dm = Design_Matrix(sampling_freq=1./TR)\n",
    "\n",
    "  for run in runs:\n",
    "    run_evs = get_run_evs(subject=subject, task_name=task_name, run=run)\n",
    "    run_dm = run_evs_to_dm(run_evs=run_evs, task_name=task_name, add_poly=add_poly, dct_basis=dct_basis)\n",
    "    task_dm = task_dm.append(run_dm)\n",
    "\n",
    "  return task_dm\n",
    "\n",
    "def load_run_timeseries(subject, task_name, run = 1, remove_mean=True, scale_ts=True):\n",
    "  \"\"\"Load timeseries data for a single subject and single run.\n",
    "  \n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    task_name (str): task name instead of the bold run\n",
    "    run (int): 1 or 2 for task runs\n",
    "    remove_mean (bool): If True, subtract the parcel-wise mean\n",
    "\n",
    "  Returns\n",
    "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
    "\n",
    "  \"\"\"\n",
    "  bold_run = task_run_dict[task_name][run-1]\n",
    "\n",
    "  bold_path = f\"{HCP_DIR}/subjects/{subject}/timeseries\"\n",
    "  bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n",
    "  ts = np.load(f\"{bold_path}/{bold_file}\")\n",
    "  if remove_mean:\n",
    "    ts -= ts.mean(axis=1, keepdims=True)\n",
    "\n",
    "  if scale_ts:\n",
    "    #scales each parcel's timeseries (instead of scaling the bold for one 1 TR from all parcels)\n",
    "    ts = scale(ts, axis=1)\n",
    "  return ts\n",
    "\n",
    "def load_task_timeseries(subject, task_name, remove_mean=True, scale_ts = True):\n",
    "  \n",
    "  runs = list(range(1,len(task_run_dict[task_name])+1))\n",
    "  task_ts = np.empty((360, 0))\n",
    "\n",
    "  for run in runs:\n",
    "    #since everything is loaded by run and scale_ts is true each parcel should be \n",
    "    #scaled for each parcel and for each run separately before being concatenated together\n",
    "    cur_run_ts = load_run_timeseries(subject=subject, task_name=task_name, run=run)\n",
    "    task_ts = np.append(task_ts, cur_run_ts, axis=1)\n",
    "  \n",
    "  return task_ts\n",
    "\n",
    "def get_sub_task_resids(subject, task_name):\n",
    " \n",
    "  #load task data\n",
    "  task_ts = load_task_timeseries(subject=subject, task_name=task_name)\n",
    "\n",
    "  #make design matrix\n",
    "  task_dm = get_task_dms(subject=subject, task_name=task_name)\n",
    "\n",
    "  #initialize empty variables to store data in\n",
    "  run_length = run_length_dict[task_name]\n",
    "  num_runs = len(task_run_dict[task_name])\n",
    "  resids = np.empty((0, num_runs*run_length))\n",
    "\n",
    "  #loop through parcels, run regression and extract residuals\n",
    "  for parcel in range(len(task_ts)):\n",
    "    model = sm.OLS(task_ts[parcel], task_dm)\n",
    "    results = model.fit()\n",
    "    cur_resids = np.array([results.resid])\n",
    "    resids = np.append(resids, cur_resids, axis=0)\n",
    "\n",
    "  #store parcel residuals in same format as original BOLD\n",
    "  out_dir = './hcp/residuals/%s'%(task_name)\n",
    "  if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "  out_fn = '%s_%s_Glasser360Cortical.npy'%(task_name, str(subject))\n",
    "\n",
    "  np.save(os.path.join(out_dir, out_fn), resids)\n",
    "\n",
    "  return resids\n",
    "\n",
    "def load_fcs(task_name, fc_type):\n",
    "  \n",
    "  base_dir = '/content/drive/My Drive/Colab Notebooks'\n",
    "\n",
    "  if fc_type == \"resid\":\n",
    "    fc_dir = os.path.join(base_dir, 'residual_fcs')\n",
    "  \n",
    "  elif fc_type == \"task\":\n",
    "    fc_dir = os.path.join(base_dir, 'task_preds_fcs')\n",
    "\n",
    "  elif fc_type == \"rest\":\n",
    "    fc_dir = os.path.join(base_dir, 'rest_fcs')\n",
    "\n",
    "  input_dir = os.path.join(fc_dir, task_name)\n",
    "  fcs_list = os.listdir(input_dir)\n",
    "  fcs = np.zeros((360, 360, len(fcs_list)))\n",
    "\n",
    "  for i, fc in enumerate(fcs_list):\n",
    "    fcs[:,:,i] = np.load(os.path.join(input_dir, fc))\n",
    "\n",
    "  return fcs\n",
    "\n",
    "def get_sub_task_pred(subject, task_name):\n",
    " \n",
    "  #load task data\n",
    "  task_ts = load_task_timeseries(subject=subject, task_name=task_name)\n",
    "\n",
    "  #make design matrix\n",
    "  task_dm = get_task_dms(subject=subject, task_name=task_name)\n",
    "  task_regs = task_dm.iloc[:,:len(conditions_dict[task_name])]\n",
    "\n",
    "  #initialize empty variables to store data in\n",
    "  run_length = run_length_dict[task_name]\n",
    "  num_runs = len(task_run_dict[task_name])\n",
    "  preds = np.empty((0, num_runs*run_length))\n",
    "\n",
    "  #loop through parcels, run regression and extract residuals\n",
    "  for parcel in range(len(task_ts)):\n",
    "    model = sm.OLS(task_ts[parcel], task_dm)\n",
    "    results = model.fit()\n",
    "    task_coefs = results.params[:len(conditions_dict[task_name])]\n",
    "    cur_preds = np.zeros(num_runs*run_length)\n",
    "    for i in range(len(conditions_dict['emotion'])):\n",
    "      cur_preds += task_coefs[i]*task_regs.iloc[:,i]\n",
    "    cur_preds = np.array(cur_preds).reshape(1, -1)\n",
    "    preds = np.append(preds, cur_preds, axis=0)\n",
    "\n",
    "  #store parcel residuals in same format as original BOLD\n",
    "  out_dir = './hcp/task_preds/%s'%(task_name)\n",
    "  if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "  out_fn = '%s_%s_Glasser360Cortical.npy'%(task_name, str(subject))\n",
    "\n",
    "  np.save(os.path.join(out_dir, out_fn), preds)\n",
    "\n",
    "  return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load region information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = np.load('./hcp_task/regions.npy').T\n",
    "region_info = dict(\n",
    "    name=regions[0].tolist(),\n",
    "    network=regions[1],\n",
    "    myelin=regions[2].astype(np.float),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkpartition_dir = pkg_resources.resource_filename('ActflowToolbox.dependencies', 'ColeAnticevicNetPartition/')\n",
    "networkdef = np.loadtxt(networkpartition_dir + '/cortex_parcel_network_assignments.txt')\n",
    "networkorder = np.asarray(sorted(range(len(networkdef)), key=lambda k: networkdef[k]))\n",
    "networkorder.shape = (len(networkorder),1)\n",
    "netorder=networkorder[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_dict = {1: 'Visual1', \n",
    "       2: 'Visual2', \n",
    "       3:'Somatomotor',\n",
    "       4:'Cingulo-Oper',\n",
    "       5:'Language',\n",
    "       6:'Default',\n",
    "       7:'Frontopariet',\n",
    "       8:'Auditory',\n",
    "       9:'Posterior-Mu',\n",
    "       10:'Dorsal-atten',\n",
    "       11:'Ventral-Mult',\n",
    "       12:'Orbito-Affec'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 1\n",
    "task_name = 'emotion'\n",
    "run = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single run GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evs = get_run_evs(subject=subject, task_name = task_name, run = run)\n",
    "run_evs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = run_evs_to_dm(run_evs, task_name = task_name)\n",
    "dm.heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_c = dm.convolve()\n",
    "dm_c.heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bold_data = load_run_timeseries(subject=subject, task_name=task_name, run=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scale(bold_data[0]), label = \"Scaled BOLD from parcel 0\")\n",
    "plt.plot(dm_c['fear_c0'], label = \"Fear condition regressor\")\n",
    "plt.plot(dm_c['neut_c0'], label = \"Neut condition regressor\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.OLS(scale(bold_data[0]), dm_c)\n",
    "res = mod.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_bold = load_task_timeseries(subject=subject, task_name=task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dm = get_task_dms(subject=subject, task_name=task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel = 0\n",
    "model = sm.OLS(task_bold[parcel], task_dm)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
